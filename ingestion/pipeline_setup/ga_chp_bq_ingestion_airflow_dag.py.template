import datetime
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator

args = {'owner': 'airflow',
        'start_date': START_DATE_AS_PY_CODE,
        'retries': 16,
        'retry_delay': datetime.timedelta(minutes=30)}

dag = DAG(dag_id='ga_chp_bq_ingestion_pipeline',
          default_args=args,
          schedule_interval='0 12 * * *')

# Do not remove the extra space at the end (the one after 'runextractor.sh')
task_1_run_extractor_cmd_parts = [
    'DAY_OF_DATA_CAPTURE={{ ds }}',
    'DEST_BQ_DATASET=bq_avro_morphl',
    'DEST_GCS_BUCKET=bq_avro_morphl',
    'docker run --rm --net host',
    '-v /opt/secrets:/opt/secrets:ro',
    '-v /opt/ga_chp_bq:/opt/ga_chp_bq:ro',
    '-v /opt/landing:/opt/landing',
    '-e DAY_OF_DATA_CAPTURE',
    '-e SRC_BQ_DATASET',
    '-e DEST_BQ_DATASET',
    '-e DEST_GCS_BUCKET',
    '-e KEY_FILE_LOCATION',
    '-e ENVIRONMENT_TYPE',
    '-e MORPHL_SERVER_IP_ADDRESS',
    '-e MORPHL_CASSANDRA_USERNAME',
    '-e MORPHL_CASSANDRA_KEYSPACE',
    '-e MORPHL_CASSANDRA_PASSWORD',
    'pysparkcontainer',
    'bash /opt/ga_chp_bq/ingestion/bq_extractor/runextractor.sh ']
task_1_run_extractor_cmd = ' '.join(task_1_run_extractor_cmd_parts)

task_1_run_extractor = BashOperator(
    task_id='task_1_run_extractor',
    bash_command=task_1_run_extractor_cmd,
    dag=dag)
