import datetime
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator

args = {'owner': 'airflow',
        'start_date': START_DATE_AS_PY_CODE,
        'retries': 16,
        'retry_delay': datetime.timedelta(minutes=30)}

dag = DAG(dag_id='ga_chp_bq_training_pipeline',
          default_args=args,
          schedule_interval='@weekly')

try:
    with open('/tmp/ga_chp_bq_training_pipeline_day_as_str.txt', 'r') as f:
        day_as_str = f.read().strip()
except:
    day_as_str = ''

try:
    with open('/tmp/ga_chp_bq_training_pipeline_unique_hash.txt', 'r') as f:
        unique_hash = f.read().strip()
except:
    unique_hash = ''

# Do not remove the extra space at the end (the one after 'runextractor.sh')
task_3_run_extractor_cmd_parts = [
    'DAY_OF_DATA_CAPTURE={{ ds }}',
    'DEST_BQ_DATASET=bq_avro_morphl',
    'DEST_GCS_BUCKET=bq_avro_morphl',
    'TRAINING_OR_PREDICTION=training',
    'TRAINING_INTERVAL=DAYS_TRAINING_INTERVAL',
    'PREDICTION_INTERVAL=DAYS_PREDICTION_INTERVAL',
    'docker run --rm --net host',
    '-v /opt/secrets:/opt/secrets:ro',
    '-v /opt/ga_chp_bq:/opt/ga_chp_bq:ro',
    '-v /opt/landing:/opt/landing',
    '-e DAY_OF_DATA_CAPTURE',
    '-e SRC_BQ_DATASET',
    '-e DEST_BQ_DATASET',
    '-e DEST_GCS_BUCKET',
    '-e TRAINING_OR_PREDICTION',
    '-e TRAINING_INTERVAL',
    '-e PREDICTION_INTERVAL',
    '-e KEY_FILE_LOCATION',
    '-e ENVIRONMENT_TYPE',
    '-e MORPHL_SERVER_IP_ADDRESS',
    '-e MORPHL_CASSANDRA_USERNAME',
    '-e MORPHL_CASSANDRA_KEYSPACE',
    '-e MORPHL_CASSANDRA_PASSWORD',
    'pysparkcontainer',
    'bash /opt/ga_chp_bq/bq_extractor/runextractor.sh ']
task_3_run_extractor_cmd = ' '.join(task_3_run_extractor_cmd_parts)

# Do not remove the extra space at the end (the one after 'runbasicpreprocessor.sh')
task_4_run_basic_preprocessor_cmd_parts = [
    f'DAY_AS_STR={day_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'TRAINING_OR_PREDICTION=training',
    'MODELS_DIR=/opt/models',
    'docker run --rm --net host',
    '-v /opt/ga_chp_bq:/opt/ga_chp_bq:ro',
    '-v /opt/models:/opt/models',
    '-e ENVIRONMENT_TYPE',
    '-e DAY_AS_STR',
    '-e UNIQUE_HASH',
    '-e TRAINING_OR_PREDICTION',
    '-e MODELS_DIR',
    '-e MORPHL_SERVER_IP_ADDRESS',
    '-e MORPHL_CASSANDRA_USERNAME',
    '-e MORPHL_CASSANDRA_KEYSPACE',
    '-e MORPHL_CASSANDRA_PASSWORD',
    'pysparkcontainer',
    'bash /opt/ga_chp_bq/pre_processing/basic_processing/runbasicpreprocessor.sh ']
task_4_run_basic_preprocessor_cmd = ' '.join(
    task_4_run_basic_preprocessor_cmd_parts)

# Do not remove the extra space at the end (the one after 'ga_chp_bq_preproc_training')
task_5_move_preproc_metadata_cmd_parts = [
    f'DAY_AS_STR={day_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'bash /opt/ga_chp_bq/pre_processing/ga_chp_bq_move_metadata.sh ga_chp_bq_preproc_training ']
task_5_move_preproc_metadata_cmd = ' '.join(
    task_5_move_preproc_metadata_cmd_parts)

# Do not remove the extra space at the end (the one after 'runadvancedpreprocessor.sh')
task_6_run_advanced_preprocessor_cmd_parts = [
    f'DAY_AS_STR={day_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'TRAINING_OR_PREDICTION=training',
    'MODELS_DIR=/opt/models',
    'docker run --rm --net host',
    '-v /opt/ga_chp_bq:/opt/ga_chp_bq:ro',
    '-v /opt/models:/opt/models',
    '-v /opt/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop:ro',
    '-e ENVIRONMENT_TYPE',
    '-e DAY_AS_STR',
    '-e UNIQUE_HASH',
    '-e TRAINING_OR_PREDICTION',
    '-e MODELS_DIR',
    '-e MORPHL_SERVER_IP_ADDRESS',
    '-e LIBHDFS3_CONF',
    'pythoncontainer',
    'bash /opt/ga_chp_bq/pre_processing/scaling_transformation/runadvancedpreprocessor.sh ']
task_6_run_advanced_preprocessor_cmd = ' '.join(
    task_6_run_advanced_preprocessor_cmd_parts)

# Do not remove the extra space at the end (the one after 'ga_chp_bq_scaled_features_training')
task_7_move_scaled_features_metadata_cmd_parts = [
    f'DAY_AS_STR={day_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'bash /opt/ga_chp_bq/pre_processing/ga_chp_bq_move_metadata.sh ga_chp_bq_scaled_features_training ']
task_7_move_scaled_features_metadata_cmd = ' '.join(
    task_7_move_scaled_features_metadata_cmd_parts)

# Do not remove the extra space at the end (the one after 'runmodelgenerator.sh')
task_8_generate_model_cmd_parts = [
    f'DAY_AS_STR={day_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'MODELS_DIR=/opt/models',
    'docker run --rm --net host',
    '-v /opt/ga_chp_bq:/opt/ga_chp_bq:ro',
    '-v /opt/models:/opt/models',
    '-v /opt/hadoop/etc/hadoop:/opt/hadoop/etc/hadoop:ro',
    '-e ENVIRONMENT_TYPE',
    '-e DAY_AS_STR',
    '-e UNIQUE_HASH',
    '-e MODELS_DIR',
    '-e MORPHL_SERVER_IP_ADDRESS',
    '-e LIBHDFS3_CONF',
    'pythoncontainer',
    'bash /opt/ga_chp_bq/training/model_generator/runmodelgenerator.sh ']
task_8_generate_model_cmd = ' '.join(task_8_generate_model_cmd_parts)

# Do not remove the extra space at the end (the one after 'ga_chp_bq_mark_model_as_valid.sh')
task_9_mark_model_as_valid_cmd_parts = [
    f'DAY_AS_STR={day_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    f'MODELS_DIR=/opt/models',
    'bash /opt/ga_chp_bq/training/pipeline_wrapup/ga_chp_bq_mark_model_as_valid.sh ']
task_9_mark_model_as_valid_cmd = ' '.join(task_9_mark_model_as_valid_cmd_parts)

# Do not remove the extra space at the end (the one after 'ga_chp_bq_generate_id_files_training.sh')
task_1_generate_id_files_training = BashOperator(
    task_id='task_1_generate_id_files_training',
    bash_command='bash /opt/ga_chp_bq/training/pipeline_setup/ga_chp_bq_generate_id_files_training.sh ',
    dag=dag)

# Do not remove the extra space at the end (the one after 'ga_chp_bq_truncate_tables_before_training_pipeline.sh')
task_2_truncate_tables = BashOperator(
    task_id='task_2_truncate_tables',
    bash_command='bash /opt/ga_chp_bq/training/pipeline_setup/ga_chp_bq_truncate_tables_before_training_pipeline.sh ',
    dag=dag)

task_3_run_extractor = BashOperator(
    task_id='task_3_run_extractor',
    bash_command=task_3_run_extractor_cmd,
    dag=dag)

task_4_run_basic_preprocessor = BashOperator(
    task_id='task_4_run_basic_preprocessor',
    bash_command=task_4_run_basic_preprocessor_cmd,
    dag=dag)

task_5_move_preproc_metadata = BashOperator(
    task_id='task_5_move_preproc_metadata',
    bash_command=task_5_move_preproc_metadata_cmd,
    dag=dag)

task_6_run_advanced_preprocessor = BashOperator(
    task_id='task_6_run_advanced_preprocessor',
    bash_command=task_6_run_advanced_preprocessor_cmd,
    dag=dag)

task_7_move_scaled_features_metadata = BashOperator(
    task_id='task_7_move_scaled_features_metadata',
    bash_command=task_7_move_scaled_features_metadata_cmd,
    dag=dag)

task_8_generate_model = BashOperator(
    task_id='task_8_generate_model',
    bash_command=task_8_generate_model_cmd,
    dag=dag)

task_9_mark_model_as_valid = BashOperator(
    task_id='task_9_mark_model_as_valid',
    bash_command=task_9_mark_model_as_valid_cmd,
    dag=dag)

task_2_truncate_tables.set_upstream(task_1_generate_id_files_training)
task_3_run_extractor.set_upstream(task_2_truncate_tables)
task_4_run_basic_preprocessor.set_upstream(task_3_run_extractor)
task_5_move_preproc_metadata.set_upstream(task_4_run_basic_preprocessor)
task_6_run_advanced_preprocessor.set_upstream(task_5_move_preproc_metadata)
task_7_move_scaled_features_metadata.set_upstream(
    task_6_run_advanced_preprocessor)
task_8_generate_model.set_upstream(task_7_move_scaled_features_metadata)
task_9_mark_model_as_valid.set_upstream(task_8_generate_model)
